{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdQrB_jXSI06"
   },
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uhUntWa2OSCY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cL4psuYSh9s"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gizeba-OSkWA"
   },
   "source": [
    "Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xU9zHyKkSjth"
   },
   "outputs": [],
   "source": [
    "# Transform the Training Set to avoid overfitting\n",
    "training_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                                        shear_range = 0.2,\n",
    "                                        zoom_range = 0.2,\n",
    "                                        horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vSDcipXKTxcn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = training_generator.flow_from_directory(\"dataset/training_set\",\n",
    "                                                      target_size = (64, 64),\n",
    "                                                      batch_size = 32,\n",
    "                                                      class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JrCEMNOTslK"
   },
   "source": [
    "Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "byuob5PlTvNS"
   },
   "outputs": [],
   "source": [
    "# Only scale the images as the training set\n",
    "test_generator = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YBmRnkIHUc7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set the target size just like the training set\n",
    "test_set = test_generator.flow_from_directory(\"dataset/test_set\",\n",
    "                                              target_size = (64, 64),\n",
    "                                              batch_size = 32,\n",
    "                                              class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_johuTSIUx49"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIirlZdPU1Bz"
   },
   "source": [
    "Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CMgbKm2AU0uH"
   },
   "outputs": [],
   "source": [
    "convolutional = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rq-R3p-VEhN"
   },
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "n_Ng9ymKVHfo"
   },
   "outputs": [],
   "source": [
    "# Kernel Size -> The size of the feature detector, n x n matrices\n",
    "# Input Shape -> (Target size, rgb: B&W = 1, colors = 3)\n",
    "convolutional.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\", input_shape = (64, 64, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shClltquV5mt"
   },
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Zhf7NbgSV8ao"
   },
   "outputs": [],
   "source": [
    "# Strides -> The Changes of each iteration\n",
    "# Pool size -> The frame in each iteration, n x n matrices\n",
    "# Padding -> To ignore cells that has no complete 2x2 form of matrices\n",
    "convolutional.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9y8-5sCWosL"
   },
   "source": [
    "Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aaJdj2bBWrYk"
   },
   "outputs": [],
   "source": [
    "convolutional.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\"))\n",
    "convolutional.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s02qtJcnWwuH"
   },
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1XxNz7MDWxdf"
   },
   "outputs": [],
   "source": [
    "convolutional.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiOLwo1XXANI"
   },
   "source": [
    "Complete Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IcG1eJUhXFxn"
   },
   "outputs": [],
   "source": [
    "# Fully Connected Graph of Neurons\n",
    "convolutional.add(tf.keras.layers.Dense(units = 128, activation = \"relu\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt2ENllUXatD"
   },
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNPSABfDXbtu"
   },
   "outputs": [],
   "source": [
    "# Set the units as 1 because it is a binary classification\n",
    "# Relu is bad to use as the activation function of the output layer\n",
    "# If Binary Classification -> Sigmoid Function\n",
    "# If Multiclass Classification -> Softmax Function\n",
    "convolutional.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4SlOMKEYCPR"
   },
   "source": [
    "# Training the Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yFhJl5FYF8K"
   },
   "source": [
    "Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SOKKbLmCYHUd"
   },
   "outputs": [],
   "source": [
    "convolutional.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4RzKrocYQzt"
   },
   "source": [
    "Training the CNN and Validating the CNN with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FS3SY3GAYREl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "252/252 [==============================] - 29s 112ms/step - loss: 0.6883 - accuracy: 0.5484 - val_loss: 0.6160 - val_accuracy: 0.6705\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - 28s 109ms/step - loss: 0.6212 - accuracy: 0.6554 - val_loss: 0.5851 - val_accuracy: 0.6915\n",
      "Epoch 3/25\n",
      "252/252 [==============================] - 26s 104ms/step - loss: 0.5689 - accuracy: 0.7030 - val_loss: 0.6318 - val_accuracy: 0.6600\n",
      "Epoch 4/25\n",
      "252/252 [==============================] - 26s 105ms/step - loss: 0.5428 - accuracy: 0.7259 - val_loss: 0.5541 - val_accuracy: 0.7255\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - 26s 104ms/step - loss: 0.5016 - accuracy: 0.7494 - val_loss: 0.4946 - val_accuracy: 0.7695\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - 26s 105ms/step - loss: 0.4857 - accuracy: 0.7605 - val_loss: 0.5164 - val_accuracy: 0.7425\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - 27s 107ms/step - loss: 0.4580 - accuracy: 0.7746 - val_loss: 0.4757 - val_accuracy: 0.7815\n",
      "Epoch 8/25\n",
      "252/252 [==============================] - 31s 124ms/step - loss: 0.4499 - accuracy: 0.7827 - val_loss: 0.4567 - val_accuracy: 0.7915\n",
      "Epoch 9/25\n",
      "252/252 [==============================] - 26s 104ms/step - loss: 0.4387 - accuracy: 0.7930 - val_loss: 0.4830 - val_accuracy: 0.7700\n",
      "Epoch 10/25\n",
      "252/252 [==============================] - 28s 112ms/step - loss: 0.4171 - accuracy: 0.8065 - val_loss: 0.5166 - val_accuracy: 0.7615\n",
      "Epoch 11/25\n",
      "252/252 [==============================] - 26s 104ms/step - loss: 0.3996 - accuracy: 0.8210 - val_loss: 0.4716 - val_accuracy: 0.7940\n",
      "Epoch 12/25\n",
      "252/252 [==============================] - 26s 102ms/step - loss: 0.3973 - accuracy: 0.8202 - val_loss: 0.4672 - val_accuracy: 0.7935\n",
      "Epoch 13/25\n",
      "252/252 [==============================] - 31s 123ms/step - loss: 0.3697 - accuracy: 0.8343 - val_loss: 0.5103 - val_accuracy: 0.7780\n",
      "Epoch 14/25\n",
      "252/252 [==============================] - 28s 111ms/step - loss: 0.3662 - accuracy: 0.8291 - val_loss: 0.4592 - val_accuracy: 0.8035\n",
      "Epoch 15/25\n",
      "252/252 [==============================] - 27s 106ms/step - loss: 0.3656 - accuracy: 0.8463 - val_loss: 0.4796 - val_accuracy: 0.7865\n",
      "Epoch 16/25\n",
      "252/252 [==============================] - 27s 109ms/step - loss: 0.3520 - accuracy: 0.8434 - val_loss: 0.4530 - val_accuracy: 0.7975\n",
      "Epoch 17/25\n",
      "252/252 [==============================] - 31s 123ms/step - loss: 0.3295 - accuracy: 0.8516 - val_loss: 0.5417 - val_accuracy: 0.7795\n",
      "Epoch 18/25\n",
      "252/252 [==============================] - 29s 113ms/step - loss: 0.3172 - accuracy: 0.8610 - val_loss: 0.4719 - val_accuracy: 0.8020\n",
      "Epoch 19/25\n",
      "252/252 [==============================] - 29s 117ms/step - loss: 0.3260 - accuracy: 0.8551 - val_loss: 0.4714 - val_accuracy: 0.8040\n",
      "Epoch 20/25\n",
      "252/252 [==============================] - 31s 122ms/step - loss: 0.3060 - accuracy: 0.8686 - val_loss: 0.4944 - val_accuracy: 0.8010\n",
      "Epoch 21/25\n",
      "252/252 [==============================] - 31s 124ms/step - loss: 0.2871 - accuracy: 0.8822 - val_loss: 0.4780 - val_accuracy: 0.8030\n",
      "Epoch 22/25\n",
      "252/252 [==============================] - 33s 131ms/step - loss: 0.2788 - accuracy: 0.8792 - val_loss: 0.5781 - val_accuracy: 0.7675\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - 36s 144ms/step - loss: 0.2567 - accuracy: 0.8903 - val_loss: 0.5242 - val_accuracy: 0.7875\n",
      "Epoch 24/25\n",
      "252/252 [==============================] - 31s 122ms/step - loss: 0.2497 - accuracy: 0.8904 - val_loss: 0.5310 - val_accuracy: 0.7940\n",
      "Epoch 25/25\n",
      "252/252 [==============================] - 31s 123ms/step - loss: 0.2383 - accuracy: 0.9025 - val_loss: 0.5332 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c02e90b9d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutional.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgpcK32WY3i0"
   },
   "source": [
    "# Predict Single Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yAP8X3aY6qa"
   },
   "source": [
    "Load the Image with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "W24cUn5XZIIb"
   },
   "outputs": [],
   "source": [
    "test_image_1 = image.load_img(\"dataset/single_prediction/cat_or_dog_1.jpg\", target_size = (64, 64))\n",
    "test_image_2 = image.load_img(\"dataset/single_prediction/cat_or_dog_2.jpg\", target_size = (64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPZNx-rWZqoF"
   },
   "source": [
    "Transform the Image to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "n0LOhiv6Zth3"
   },
   "outputs": [],
   "source": [
    "test_image_1 = image.img_to_array(test_image_1)\n",
    "test_image_2 = image.img_to_array(test_image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkIRz90haDCZ"
   },
   "source": [
    "Add Extra Dimension as the Batch in the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9Yfzk1Y2aIf-"
   },
   "outputs": [],
   "source": [
    "test_image_1 = np.expand_dims(test_image_1, axis = 0)\n",
    "test_image_2 = np.expand_dims(test_image_2, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WzUf0B4agvt"
   },
   "source": [
    "Check Each Class Indices of the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3mVyZcR6akty"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUQcZEeYaRlv"
   },
   "source": [
    "Making New Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "kdczLYuuaTNE"
   },
   "outputs": [],
   "source": [
    "prediction1 = convolutional.predict(test_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-3HjLilwaY5C"
   },
   "outputs": [],
   "source": [
    "prediction2 = convolutional.predict(test_image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5v9C371au1b"
   },
   "source": [
    "Get the Result from the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "lK5vI-x5axzS"
   },
   "outputs": [],
   "source": [
    "def check_pred_from_batch(prediction_batch):\n",
    "  if(prediction_batch[0][0] == 1):\n",
    "    print(\"Dog\")\n",
    "  else:\n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "V8cqmjbQbCu-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "check_pred_from_batch(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vrgMblWzbDWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "check_pred_from_batch(prediction2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Convolutional Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
